# -*- coding: utf-8 -*-
"""faceinpainting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vzeclnCvrk98FmvpvyooTHwOghkk7fG8
"""

import argparse
import logging
import sys
import torch
import os
from model.KPN_2_deblur import ImageInpainter
import utils.helpers
import utils.data_loaders

from utils.average_meter import AverageMeter
from datetime import datetime
from time import time
from tensorboardX import SummaryWriter

import numpy as np

import lpips
from utils.ssim_loss import SSIM_Loss

def get_args_from_command_line():
    parser = argparse.ArgumentParser(description='The argument parser of runner')
    parser.add_argument('--config', dest='config', help='Dataset config file', defaule='ffhq_square')
    parser.add_argument('--test', dest='test', help='Test neural networks', action='store_true')
    parser.add_argument('--weights', dest='weights', help='Initialize network from the weights file', default=None)
    parser.add_argument('--init_lr', dest='init_lr', help='Init lr', default=None)
    args = parser.parse_known_args()[0]
    return args


def train_net(cfg):
    # Enable the inbuilt cudnn auto-tuner to find the best algorithm to use
    torch.backends.cudnn.benchmark = True
    train_dataset_loader = utils.data_loaders.DATASET_LOADER_MAPPING[cfg.dataset.train](cfg)
    test_dataset_loader = utils.data_loaders.DATASET_LOADER_MAPPING[cfg.dataset.test](cfg)
    train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset_loader.get_dataset('train'),
                                                    batch_size=cfg.train.batch_size,
                                                    num_workers=cfg.const.num_workers,
                                                    collate_fn=utils.data_loaders.collate_fn,
                                                    pin_memory=True,
                                                    shuffle=True,
                                                    drop_last=True)
    val_data_loader = torch.utils.data.DataLoader(dataset=test_dataset_loader.get_dataset('val'),
                                                  batch_size=1,
                                                  num_workers=cfg.const.num_workers,
                                                  collate_fn=utils.data_loaders.collate_fn,
                                                  pin_memory=True,
                                                  shuffle=False)

    # Set up folders for logs and checkpoints
    # output_dir = os.path.join(cfg.DIR.OUT_PATH, '%s', datetime.now().isoformat())
    output_dir = cfg.dir.out_path + '/%s/' + datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    cfg.dir.checkpoints = output_dir % 'checkpoints'
    cfg.dir.logs = output_dir % 'logs'
    if not os.path.exists(cfg.dir.checkpoints):
        os.makedirs(cfg.dir.checkpoints)

    # Create tensorboard writers
    train_writer = SummaryWriter(os.path.join(cfg.dir.logs, 'train'))
    val_writer = SummaryWriter(os.path.join(cfg.dir.logs, 'test'))

    net = ImageInpainter()
    net.apply(utils.helpers.init_weights)
    logging.debug('Parameters in Net: %d.' % utils.helpers.count_parameters(net))

    # Move the network to GPU if possible
    if torch.cuda.is_available():
        net = torch.nn.DataParallel(net).cuda()

    # Create the optimizers
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()),
                                 lr=cfg.train.init_lr,
                                 betas=cfg.train.betas)
    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.train.lr_milestones,
                                                        gamma=cfg.train.gamma)

    loss_func = torch.nn.L1Loss()
    ssim = SSIM_Loss()
    percept = lpips.PerceptualLoss(
        model="net-lin", net="vgg", use_gpu="cuda"
    )

    init_epoch = 0
    best_metrics = float('inf')
    if 'weights' in cfg.const:
        logging.info('Recovering from %s ...' % (cfg.const.weights))
        checkpoint = torch.load(cfg.const.weights)
        best_metrics = checkpoint['best_metrics']
        net.load_state_dict(checkpoint['grnet'])
        epoch_idx = checkpoint['epoch_index']
        logging.info('Recover complete. Current epoch = #%d; best metrics = %s.' % (init_epoch, best_metrics))

    for epoch_idx in range(init_epoch + 1, cfg.train.epochs + 1):
        epoch_start_time = time()

        batch_time = AverageMeter()
        data_time = AverageMeter()

        net.train()
        batch_end_time = time()
        losses = AverageMeter(['Generator_L1', 'Denoiser_L1', 'Perceptual', 'SSIM'])
        n_batches = len(train_data_loader)
        for batch_idx, data in enumerate(train_data_loader):
            data_time.update(time() - batch_end_time)
            for k, v in data.items():
                data[k] = utils.helpers.var_or_cuda(v)

            generated, denoised = net(data['input'])
            loss_1 = 6. * loss_func(generated * data['mask'], data['gt'] * data['mask']) + \
                     loss_func(generated * (1 - data['mask']), data['gt'] * (1 - data['mask']))
            loss_2 = 6. * loss_func(denoised * data['mask'], data['gt'] * data['mask']) + \
                     loss_func(denoised * (1 - data['mask']), data['gt'] * (1 - data['mask']))
            perc_loss = percept(denoised, data['gt']).sum()
            ssim_loss = ssim(data['gt'], denoised)
            loss = loss_1 + loss_2 + 0.1 * perc_loss + 0.5 * ssim_loss
            losses.update([loss_1.item() * 1000, loss_2.item() * 1000, perc_loss.item()*1000, ssim_loss.item()*1000])

            net.zero_grad()
            loss.backward()
            optimizer.step()

            n_itr = (epoch_idx - 1) * n_batches + batch_idx
            train_writer.add_scalar('Loss/Batch/Generator_L1', loss_1.item() * 1000, n_itr)
            train_writer.add_scalar('Loss/Batch/Denoiser_L1', loss_2.item() * 1000, n_itr)
            train_writer.add_scalar('Loss/Batch/Perceptual_Loss', perc_loss.item() * 1000, n_itr)
            train_writer.add_scalar('Loss/Batch/SSIM_Loss', ssim_loss.item() * 1000, n_itr)

            batch_time.update(time() - batch_end_time)
            batch_end_time = time()
            logging.info('[Epoch %d/%d][Batch %d/%d] BatchTime = %.3f (s) DataTime = %.3f (s) Losses = %s' %
                         (epoch_idx, cfg.train.epochs, batch_idx + 1, n_batches, batch_time.val(), data_time.val(),
                          ['%.4f' % l for l in losses.val()]))

        lr_scheduler.step()
        epoch_end_time = time()
        train_writer.add_scalar('Loss/Epoch/Generator_L1', losses.avg(0), epoch_idx)
        train_writer.add_scalar('Loss/Epoch/Denoiser_L1', losses.avg(1), epoch_idx)
        train_writer.add_scalar('Loss/Epoch/Perceptual_Loss', losses.avg(2), epoch_idx)
        train_writer.add_scalar('Loss/Epoch/SSIM_Loss', losses.avg(3), epoch_idx)
        logging.info(
            '[Epoch %d/%d] EpochTime = %.3f (s) Losses = %s' %
            (epoch_idx, cfg.train.epochs, epoch_end_time - epoch_start_time, ['%.4f' % l for l in losses.avg()]))

        # Validate the current model
        metrics = test_net(cfg, epoch_idx, val_data_loader, val_writer, net)

        # Save ckeckpoints
        if epoch_idx % cfg.train.save_freq == 0 or metrics < best_metrics:
            file_name = 'ckpt-best.pth' if metrics < best_metrics else 'ckpt-epoch-%03d.pth' % epoch_idx
            output_path = os.path.join(cfg.dir.checkpoints, file_name)
            print(output_path)
            torch.save({
                'epoch_index': epoch_idx,
                'best_metrics': metrics,
                'grnet': net.state_dict()
            }, output_path)  # yapf: disable

            logging.info('Saved checkpoint to %s ...' % output_path)
            if metrics < best_metrics:
                best_metrics = metrics

    train_writer.close()
    val_writer.close()
    return


def test_net(cfg, epoch_idx=-1, test_data_loader=None, test_writer=None, net=None):
    # Enable the inbuilt cudnn auto-tuner to find the best algorithm to use
    torch.backends.cudnn.benchmark = True

    if test_data_loader is None:
        dataset_loader = utils.data_loaders.DATASET_LOADER_MAPPING[cfg.dataset.test](cfg)
        test_data_loader = torch.utils.data.DataLoader(dataset=dataset_loader.get_dataset('test'),
                                                       batch_size=1,
                                                       num_workers=cfg.const.num_workers,
                                                       collate_fn=utils.data_loaders.collate_fn,
                                                       pin_memory=True,
                                                       shuffle=False)

    # Setup networks and initialize networks
    if net is None:
        net = ImageInpainter()
        if torch.cuda.is_available():
            net = torch.nn.DataParallel(net).cuda()

        logging.info('Recovering from %s ...' % (cfg.const.weights))
        checkpoint = torch.load(cfg.const.weights)
        net.load_state_dict(checkpoint['grnet'])

    net.eval()

    # Testing loop
    n_samples = len(test_data_loader)
    loss_func = torch.nn.MSELoss()
    ssim_loss = SSIM_Loss()
    test_losses = AverageMeter(['Generator_MSE', 'Denoiser_MSE', 'PSNR', 'ssim'])

    for img_idx, data in enumerate(test_data_loader):
        with torch.no_grad():
            for k, v in data.items():
                data[k] = utils.helpers.var_or_cuda(v)
            generated, denoised = net(data['input'])
            loss_1 = loss_func(generated, data['gt'])
            loss_2 = loss_func(denoised, data['gt'])
            loss_3 = ssim_loss(data['gt'], denoised)

            psnr = 10. * torch.log10(1. / loss_2)  # batch size 1

            test_losses.update([loss_1.item() * 1000, loss_2.item() * 1000, psnr.item(), 1 - loss_3.item()])

            generated_img = generated * data['mask'] + data['gt'] * (1 - data['mask'])
            generated_img = np.clip(generated_img.squeeze().cpu().numpy(), 0, 1)

            denoised_img = denoised * data['mask'] + data['gt'] * (1 - data['mask'])
            denoised_img = np.clip(denoised_img.squeeze().cpu().numpy(), 0, 1)

            if test_writer is not None and img_idx in [100, 200, 300, 400, 500]:
                test_writer.add_image('Model%02d/Ori/Input' % img_idx, data['input'].squeeze().cpu().numpy(), epoch_idx,
                                      dataformats='CHW')
                test_writer.add_image('Model%02d/Ori/Inpaint' % img_idx, generated_img, epoch_idx,
                                      dataformats='CHW')
                test_writer.add_image('Model%02d/Ori/Denoised' % img_idx, denoised_img, epoch_idx,
                                      dataformats='CHW')
                test_writer.add_image('Model%02d/Ori/Groundtruth' % img_idx, data['gt'].squeeze().cpu().numpy(), epoch_idx,
                                      dataformats='CHW')

            logging.info('Test[%d/%d] Losses = %s ' %
                         (img_idx + 1, n_samples, ['%.4f' % l for l in test_losses.val()]))


    print('Loss/Epoch/MSE: %f' % test_losses.avg(1))
    print('Loss/Epoch/PSNR: %f' % test_losses.avg(2))
    print('Loss/Epoch/SSIM: %f' % test_losses.avg(3))

    # Add testing results to TensorBoard
    if test_writer is not None:
        test_writer.add_scalar('Loss/Epoch/Generator_MSE', test_losses.avg(0), epoch_idx)
        test_writer.add_scalar('Loss/Epoch/Denoiser_MSE', test_losses.avg(1), epoch_idx)
        test_writer.add_scalar('Loss/Epoch/PSNR', test_losses.avg(2), epoch_idx)
        test_writer.add_scalar('Loss/Epoch/SSIM', test_losses.avg(3), epoch_idx)

    return test_losses.avg(1)


if __name__ == '__main__':
    logging.basicConfig(format='[%(levelname)s] %(asctime)s %(message)s', level=logging.DEBUG)
    args = get_args_from_command_line()
    if args.config == 'ffhq_irregular':
        from configs.config_ffhq_irregular_256 import cfg
    elif args.config == 'paris':
        from configs.config_paris_irregular_256 import cfg
    else:
        from configs.config_ffhq_square_128 import cfg
    os.environ["CUDA_VISIBLE_DEVICES"] = cfg.const.device
    if not args.test:
        if args.weights:
            cfg.const.weights = args.weights
        if args.init_lr:
            cfg.train.init_lr = float(args.init_lr)
        train_net(cfg)
    else:
        cfg.const.weights = args.weights
        test_net(cfg)
